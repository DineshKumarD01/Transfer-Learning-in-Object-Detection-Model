{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fb8d06",
   "metadata": {},
   "source": [
    "###### CREATION OF TWO CUSTOM LAYERS IN TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc3fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#powerful library for building and training machine learning models\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2385f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"addition_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_a (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_b (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " custom_add (CustomAddLayer  (None, 10)                   0         ['input_a[0][0]',             \n",
      " )                                                                   'input_b[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"multiplication_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_a (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_b (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " custom_multiply (CustomMul  (None, 10)                   0         ['input_a[0][0]',             \n",
      " tiplyLayer)                                                         'input_b[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#define two custom layers, one for addition (AddLayer) and one for multiplication (MultiplyLayer).\n",
    "#These layers perform basic mathematical operations on their inputs.\n",
    "\n",
    "class CustomAddLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='custom_add_layer', **kwargs):\n",
    "        super(CustomAddLayer, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No additional weights or trainable parameters needed for addition\n",
    "        super(CustomAddLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Element-wise addition\n",
    "        return tf.add(inputs[0], inputs[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomAddLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "class CustomMultiplyLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='custom_multiply_layer', **kwargs):\n",
    "        super(CustomMultiplyLayer, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No additional weights or trainable parameters needed for multiplication\n",
    "        super(CustomMultiplyLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Element-wise multiplication\n",
    "        return tf.multiply(inputs[0], inputs[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomMultiplyLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "# Example usage:\n",
    "input_a = tf.keras.layers.Input(shape=(10,), name='input_a')\n",
    "input_b = tf.keras.layers.Input(shape=(10,), name='input_b')\n",
    "\n",
    "# Adding custom layers\n",
    "add_result = CustomAddLayer(name='custom_add')([input_a, input_b])\n",
    "multiply_result = CustomMultiplyLayer(name='custom_multiply')([input_a, input_b])\n",
    "\n",
    "# Creating models\n",
    "add_model = tf.keras.Model(inputs=[input_a, input_b], outputs=add_result, name='addition_model')\n",
    "multiply_model = tf.keras.Model(inputs=[input_a, input_b], outputs=multiply_result, name='multiplication_model')\n",
    "\n",
    "# Displaying model summaries\n",
    "add_model.summary()\n",
    "multiply_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d121d3",
   "metadata": {},
   "source": [
    "######  COMMBINATION OF TWO LAYERS IN A THIRD CUSTOM LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe76a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combined_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_a (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_b (InputLayer)        [(None, 10)]                 0         []                            \n",
      "                                                                                                  \n",
      " custom_add (CustomAddLayer  (None, 10)                   0         ['input_a[0][0]',             \n",
      " )                                                                   'input_b[0][0]']             \n",
      "                                                                                                  \n",
      " custom_multiply (CustomMul  (None, 10)                   0         ['input_a[0][0]',             \n",
      " tiplyLayer)                                                         'input_b[0][0]']             \n",
      "                                                                                                  \n",
      " custom_combine (CustomComb  (None, 20)                   0         ['custom_add[0][0]',          \n",
      " ineLayer)                                                           'custom_multiply[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Combination of  these two layers in a third custom layer. Concatenate them or multiply them\n",
    "class CustomCombineLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, name='custom_combine_layer', **kwargs):\n",
    "        super(CustomCombineLayer, self).__init__(name=name, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # No additional weights or trainable parameters needed for combination\n",
    "        super(CustomCombineLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        add_output = inputs[0]\n",
    "        multiply_output = inputs[1]\n",
    "\n",
    "        # Concatenate the outputs along the last axis (axis=-1)\n",
    "        combined_output = tf.concat([add_output, multiply_output], axis=-1)\n",
    "\n",
    "        return combined_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomCombineLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "# Example usage:\n",
    "input_a = tf.keras.layers.Input(shape=(10,), name='input_a')\n",
    "input_b = tf.keras.layers.Input(shape=(10,), name='input_b')\n",
    "\n",
    "# Adding custom layers\n",
    "add_result = CustomAddLayer(name='custom_add')([input_a, input_b])\n",
    "multiply_result = CustomMultiplyLayer(name='custom_multiply')([input_a, input_b])\n",
    "\n",
    "# Combine custom layers using the third custom layer\n",
    "combined_result = CustomCombineLayer(name='custom_combine')([add_result, multiply_result])\n",
    "\n",
    "# Creating model\n",
    "combined_model = tf.keras.Model(inputs=[input_a, input_b], outputs=combined_result, name='combined_model')\n",
    "\n",
    "# Displaying model summary\n",
    "combined_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca227f3",
   "metadata": {},
   "source": [
    "###### CREATE A MODEL AND OBSERVE WORKING OF BATCH INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca159fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 386ms/step\n",
      "Input Data A:\n",
      "[[5.47663010e-01 1.15408703e-02 2.97959456e-01 8.62817715e-01\n",
      "  3.23037386e-01 7.88809179e-02 1.50127267e-01 7.01642161e-01\n",
      "  5.40703643e-01 7.58603524e-01]\n",
      " [1.46031640e-01 7.26376017e-01 1.29225607e-01 7.91798559e-01\n",
      "  3.30847765e-01 9.37856242e-01 8.11248222e-02 4.84631185e-01\n",
      "  1.32648690e-01 1.75552140e-01]\n",
      " [9.75075589e-01 3.27718277e-01 7.41454458e-01 8.93303360e-01\n",
      "  8.66129356e-01 5.82422234e-01 6.44035308e-01 3.19852439e-02\n",
      "  6.51189577e-01 1.25405941e-01]\n",
      " [3.49154310e-01 6.93716960e-02 3.70893829e-02 7.52512071e-01\n",
      "  2.92611278e-01 1.59475870e-01 1.48923400e-01 7.17888784e-01\n",
      "  7.70432230e-01 9.64849477e-04]\n",
      " [8.46723234e-01 6.94034274e-01 9.29336363e-01 5.57503154e-01\n",
      "  9.10085153e-01 9.73873335e-01 4.03440250e-01 4.74969127e-01\n",
      "  7.93314529e-01 4.08557161e-01]]\n",
      "\n",
      "Input Data B:\n",
      "[[0.4620052  0.83813497 0.31789454 0.03236751 0.62853236 0.53764737\n",
      "  0.52758798 0.68615416 0.33593994 0.02549692]\n",
      " [0.46205322 0.1223988  0.3816208  0.82908519 0.87228945 0.59967562\n",
      "  0.40445801 0.15057094 0.82749883 0.99753633]\n",
      " [0.50460112 0.14598592 0.40062835 0.29615451 0.34392949 0.75346883\n",
      "  0.27389442 0.2102609  0.32611906 0.4959244 ]\n",
      " [0.26662671 0.27094258 0.1593272  0.72952073 0.06874106 0.73891339\n",
      "  0.61879275 0.38002851 0.63800604 0.71457378]\n",
      " [0.84685411 0.7098185  0.85422568 0.03928024 0.05953868 0.1063662\n",
      "  0.67349531 0.92702305 0.58488251 0.1670199 ]]\n",
      "\n",
      "Predictions:\n",
      "[[1.00966823e+00 8.49675834e-01 6.15854025e-01 8.95185232e-01\n",
      "  9.51569736e-01 6.16528273e-01 6.77715302e-01 1.38779640e+00\n",
      "  8.76643598e-01 7.84100413e-01 2.53023177e-01 9.67280660e-03\n",
      "  9.47196856e-02 2.79272627e-02 2.03039452e-01 4.24101204e-02\n",
      "  7.92053416e-02 4.81434703e-01 1.81643948e-01 1.93420500e-02]\n",
      " [6.08084857e-01 8.48774791e-01 5.10846376e-01 1.62088370e+00\n",
      "  1.20313728e+00 1.53753185e+00 4.85582829e-01 6.35202110e-01\n",
      "  9.60147560e-01 1.17308843e+00 6.74743876e-02 8.89075547e-02\n",
      "  4.93151806e-02 6.56468451e-01 2.88595021e-01 5.62409520e-01\n",
      "  3.28115821e-02 7.29713738e-02 1.09766640e-01 1.75119638e-01]\n",
      " [1.47967672e+00 4.73704219e-01 1.14208281e+00 1.18945789e+00\n",
      "  1.21005881e+00 1.33589101e+00 9.17929709e-01 2.42246151e-01\n",
      "  9.77308631e-01 6.21330380e-01 4.92024243e-01 4.78422530e-02\n",
      "  2.97047704e-01 2.64555812e-01 2.97887444e-01 4.38836992e-01\n",
      "  1.76397681e-01 6.72524655e-03 2.12365329e-01 6.21918663e-02]\n",
      " [6.15781069e-01 3.40314269e-01 1.96416587e-01 1.48203278e+00\n",
      "  3.61352324e-01 8.98389280e-01 7.67716169e-01 1.09791732e+00\n",
      "  1.40843821e+00 7.15538621e-01 9.30938721e-02 1.87957454e-02\n",
      "  5.90934744e-03 5.48973203e-01 2.01144088e-02 1.17838867e-01\n",
      "  9.21527222e-02 2.72818208e-01 4.91540402e-01 6.89456181e-04]\n",
      " [1.69357729e+00 1.40385270e+00 1.78356206e+00 5.96783400e-01\n",
      "  9.69623804e-01 1.08023953e+00 1.07693553e+00 1.40199220e+00\n",
      "  1.37819695e+00 5.75577021e-01 7.17051029e-01 4.92638350e-01\n",
      "  7.93862998e-01 2.18988601e-02 5.41852713e-02 1.03587195e-01\n",
      "  2.71715105e-01 4.40307319e-01 4.63995785e-01 6.82371706e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating random input data for batch inference\n",
    "num_samples = 5\n",
    "input_data_a = np.random.rand(num_samples, 10)\n",
    "input_data_b = np.random.rand(num_samples, 10)\n",
    "\n",
    "# Performing batch inference\n",
    "predictions = combined_model.predict([input_data_a, input_data_b])\n",
    "\n",
    "# Displaying predictions\n",
    "print(\"Input Data A:\")\n",
    "print(input_data_a)\n",
    "print(\"\\nInput Data B:\")\n",
    "print(input_data_b)\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10338cd0",
   "metadata": {},
   "source": [
    "###### SPLITTING OF INPUT IMAGE INTO 4*4 TILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0904f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " tf.image.extract_patches (  (None, None, None, 48)    0         \n",
      " TFOpLambda)                                                     \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 4, 4, 3)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Input Image:\n",
      "[[[ 0  2  0]\n",
      "  [-1  1 -2]\n",
      "  [-1  2  1]\n",
      "  [-1 -1  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 1 -1  0]\n",
      "  [ 1 -1 -1]\n",
      "  [ 1  0 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1  2]\n",
      "  [-1 -1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[-1  0  1]\n",
      "  [ 0  1  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0  1  1]\n",
      "  [ 1  0  0]\n",
      "  [ 0 -1 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1 -1]\n",
      "  [-1  0  0]\n",
      "  [ 0  1  1]\n",
      "  [ 0 -1  1]\n",
      "  [ 1  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0  3 -2]]\n",
      "\n",
      " [[ 1  0  0]\n",
      "  [ 2  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [-1  0 -1]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0  1]\n",
      "  [ 0 -2  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  0  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 1 -1  1]]\n",
      "\n",
      " [[ 2  0 -1]\n",
      "  [ 1  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  0]\n",
      "  [ 1 -1  0]\n",
      "  [ 1  1  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0 -1  2]\n",
      "  [-1 -1 -1]\n",
      "  [-2  1  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1  1]\n",
      "  [ 1  0 -1]]\n",
      "\n",
      " [[ 0  0  1]\n",
      "  [-2  0  1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  1  0]\n",
      "  [ 0  0  0]\n",
      "  [-1  1  0]\n",
      "  [ 0  1  0]\n",
      "  [-1  0 -1]\n",
      "  [ 1  0  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 1  1  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 0 -1  1]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 1  0  0]\n",
      "  [ 0  0  2]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0  0]\n",
      "  [-1  1  0]\n",
      "  [ 0  1 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 1  0 -1]\n",
      "  [ 0  1 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0  2  0]\n",
      "  [-1  0 -1]\n",
      "  [-1  0 -1]]\n",
      "\n",
      " [[ 0  0 -1]\n",
      "  [ 1  0 -1]\n",
      "  [ 2  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [-1  0  0]\n",
      "  [ 0 -2  0]\n",
      "  [ 0  1  0]\n",
      "  [-2  0  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0 -1]\n",
      "  [ 0 -1  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0  1  0]]\n",
      "\n",
      " [[ 0  1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  0]\n",
      "  [-1  1 -1]\n",
      "  [ 1  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [-1  0  1]\n",
      "  [ 0 -1  0]\n",
      "  [ 2  0 -1]\n",
      "  [ 0  0  1]\n",
      "  [ 0 -1  1]\n",
      "  [-1  2 -1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[-1  2  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  1  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0 -1  0]\n",
      "  [-1  1  1]\n",
      "  [ 0  0  1]\n",
      "  [-1  0  0]\n",
      "  [-1 -1  0]\n",
      "  [ 0  1  0]]\n",
      "\n",
      " [[ 1  0  0]\n",
      "  [ 2  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0 -1  0]\n",
      "  [-2  1  0]\n",
      "  [-2  0 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0 -1  1]\n",
      "  [-2  1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0  1  0]\n",
      "  [-2  0  1]]\n",
      "\n",
      " [[-1  0  0]\n",
      "  [ 2  0  0]\n",
      "  [ 0 -1  1]\n",
      "  [ 1  0  1]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0  1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0 -1 -1]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  0]\n",
      "  [ 1  0  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  1 -1]\n",
      "  [ 2  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [-2  1  1]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  0  0]\n",
      "  [-1 -1  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0 -2  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0 -1 -1]\n",
      "  [ 0  0  1]\n",
      "  [-1  1 -1]\n",
      "  [ 0 -1 -1]\n",
      "  [ 0  1  1]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 1  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  2 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1  0]\n",
      "  [ 1 -1  1]\n",
      "  [ 0  0 -2]\n",
      "  [ 0  2  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  0  0]\n",
      "  [-1  0  1]]\n",
      "\n",
      " [[ 0  0 -1]\n",
      "  [ 0 -1  0]\n",
      "  [ 1  0  1]\n",
      "  [ 0  1 -1]\n",
      "  [ 0 -1  1]\n",
      "  [ 0  1  0]\n",
      "  [ 0  1  1]\n",
      "  [ 1  0  0]\n",
      "  [ 0 -1  0]\n",
      "  [-1  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  1]\n",
      "  [ 0  1  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 1  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  1  0]\n",
      "  [ 0 -1  1]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  1 -1]\n",
      "  [-1  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  1  0]\n",
      "  [ 0  0  1]]\n",
      "\n",
      " [[ 0 -1 -1]\n",
      "  [ 0  0  0]\n",
      "  [ 0  1  2]\n",
      "  [ 1  0  0]\n",
      "  [-1  0  0]\n",
      "  [ 0 -1  0]\n",
      "  [ 1 -1 -1]\n",
      "  [ 0  1  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0 -1]\n",
      "  [ 0  1 -1]\n",
      "  [ 0 -1  0]\n",
      "  [ 0 -1  0]\n",
      "  [-2  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  1]]]\n",
      "\n",
      "Result Tiles:\n",
      "[[[[ 0  2  0]\n",
      "   [-1  1 -2]\n",
      "   [-1  2  1]\n",
      "   [-1 -1  0]]\n",
      "\n",
      "  [[-1  0  1]\n",
      "   [ 0  1  0]\n",
      "   [ 0  1  0]\n",
      "   [ 0  1  1]]\n",
      "\n",
      "  [[ 1  0  0]\n",
      "   [ 2  0  0]\n",
      "   [ 0  0  1]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 2  0 -1]\n",
      "   [ 1  0  0]\n",
      "   [ 0  0  1]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0 -1  0]\n",
      "   [ 1 -1  0]\n",
      "   [ 1 -1 -1]\n",
      "   [ 1  0 -1]]\n",
      "\n",
      "  [[ 1  0  0]\n",
      "   [ 0 -1 -1]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0  0 -1]\n",
      "   [-1  0 -1]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  0  1]]\n",
      "\n",
      "  [[ 1 -1  0]\n",
      "   [ 1  1  0]\n",
      "   [ 0 -1  0]\n",
      "   [ 1  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0]\n",
      "   [ 0  0  1]\n",
      "   [ 0 -1  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  1 -1]\n",
      "   [-1  0  0]\n",
      "   [ 0  1  1]]\n",
      "\n",
      "  [[ 0 -2  0]\n",
      "   [-1  0  0]\n",
      "   [ 0  0  0]\n",
      "   [-1  0  0]]\n",
      "\n",
      "  [[ 0 -1  2]\n",
      "   [-1 -1 -1]\n",
      "   [-2  1  0]\n",
      "   [ 0 -1  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  1  2]\n",
      "   [-1 -1  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0 -1  1]\n",
      "   [ 1  0  0]\n",
      "   [ 1  0  0]\n",
      "   [ 0  3 -2]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 1 -1  1]]\n",
      "\n",
      "  [[ 1  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  1  1]\n",
      "   [ 1  0 -1]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  1]\n",
      "   [-2  0  1]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0 -1]]\n",
      "\n",
      "  [[ 1  0  0]\n",
      "   [ 0  0  2]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  0 -1]]\n",
      "\n",
      "  [[ 0  0 -1]\n",
      "   [ 1  0 -1]\n",
      "   [ 2  0  0]\n",
      "   [ 1  0  0]]\n",
      "\n",
      "  [[ 0  1  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  0  1]]]\n",
      "\n",
      "\n",
      " [[[ 0  1  0]\n",
      "   [ 0  0  0]\n",
      "   [-1  1  0]\n",
      "   [ 0  1  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [-1  1  0]\n",
      "   [ 0  1 -1]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0  0 -1]\n",
      "   [-1  0  0]\n",
      "   [ 0 -2  0]\n",
      "   [ 0  1  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [-1  1 -1]\n",
      "   [ 1  0  0]\n",
      "   [ 0  0 -1]]]\n",
      "\n",
      "\n",
      " [[[-1  0 -1]\n",
      "   [ 1  0  0]\n",
      "   [ 0  1  0]\n",
      "   [ 0 -1  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 1  0 -1]\n",
      "   [ 0  1 -1]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[-2  0  0]\n",
      "   [ 0 -1  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0 -1]]\n",
      "\n",
      "  [[-1  0  1]\n",
      "   [ 0 -1  0]\n",
      "   [ 2  0 -1]\n",
      "   [ 0  0  1]]]\n",
      "\n",
      "\n",
      " [[[ 1  1  0]\n",
      "   [ 0 -1  0]\n",
      "   [ 0 -1  1]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0  1  0]\n",
      "   [ 0  2  0]\n",
      "   [-1  0 -1]\n",
      "   [-1  0 -1]]\n",
      "\n",
      "  [[ 0  0 -1]\n",
      "   [ 0 -1  0]\n",
      "   [ 1  0  0]\n",
      "   [ 0  1  0]]\n",
      "\n",
      "  [[ 0 -1  1]\n",
      "   [-1  2 -1]\n",
      "   [ 0 -1  0]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[-1  2  0]\n",
      "   [ 0 -1  0]\n",
      "   [ 0  0  1]\n",
      "   [ 0  1  0]]\n",
      "\n",
      "  [[ 1  0  0]\n",
      "   [ 2  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0 -1  0]]\n",
      "\n",
      "  [[-1  0  0]\n",
      "   [ 2  0  0]\n",
      "   [ 0 -1  1]\n",
      "   [ 1  0  1]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [-2  1  1]\n",
      "   [ 0  0 -1]]]\n",
      "\n",
      "\n",
      " [[[-1  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  1]]\n",
      "\n",
      "  [[-2  1  0]\n",
      "   [-2  0 -1]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  1]]\n",
      "\n",
      "  [[ 0  0 -1]\n",
      "   [ 0  0  1]\n",
      "   [ 0 -1  0]\n",
      "   [ 0 -1 -1]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [-1 -1  0]\n",
      "   [ 0  1  0]\n",
      "   [ 0  1  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0]\n",
      "   [ 1  0  0]\n",
      "   [ 0 -1  0]\n",
      "   [-1  1  1]]\n",
      "\n",
      "  [[ 0 -1  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0 -1  1]\n",
      "   [-2  1  0]]\n",
      "\n",
      "  [[ 0  0  1]\n",
      "   [ 0  0  0]\n",
      "   [ 1  0  0]\n",
      "   [-1  0  0]]\n",
      "\n",
      "  [[ 0 -2  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0 -1 -1]\n",
      "   [ 0  0  1]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  1]\n",
      "   [-1  0  0]\n",
      "   [-1 -1  0]\n",
      "   [ 0  1  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 1  0  0]\n",
      "   [ 0  1  0]\n",
      "   [-2  0  1]]\n",
      "\n",
      "  [[ 0  1 -1]\n",
      "   [ 2  0  0]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[-1  1 -1]\n",
      "   [ 0 -1 -1]\n",
      "   [ 0  1  1]\n",
      "   [ 0  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0]\n",
      "   [ 0  1  0]\n",
      "   [ 0  0  0]\n",
      "   [ 1  0  0]]\n",
      "\n",
      "  [[ 0  0 -1]\n",
      "   [ 0 -1  0]\n",
      "   [ 1  0  1]\n",
      "   [ 0  1 -1]]\n",
      "\n",
      "  [[ 1  0  0]\n",
      "   [ 0  0 -1]\n",
      "   [ 0 -1  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0 -1 -1]\n",
      "   [ 0  0  0]\n",
      "   [ 0  1  2]\n",
      "   [ 1  0  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  1]\n",
      "   [ 0 -1  0]\n",
      "   [ 0  2 -1]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0 -1  1]\n",
      "   [ 0  1  0]\n",
      "   [ 0  1  1]\n",
      "   [ 1  0  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  1  0]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  1  0]]\n",
      "\n",
      "  [[-1  0  0]\n",
      "   [ 0 -1  0]\n",
      "   [ 1 -1 -1]\n",
      "   [ 0  1  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  0  0]\n",
      "   [ 0  1  0]\n",
      "   [ 1 -1  1]\n",
      "   [ 0  0 -2]]\n",
      "\n",
      "  [[ 0 -1  0]\n",
      "   [-1  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0 -1  1]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  1 -1]\n",
      "   [-1  0  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  1 -1]\n",
      "   [ 0 -1  0]]]\n",
      "\n",
      "\n",
      " [[[ 0  2  0]\n",
      "   [ 0  0  1]\n",
      "   [ 0  0  0]\n",
      "   [-1  0  1]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  0  1]\n",
      "   [ 0  1  0]\n",
      "   [ 0  0  0]]\n",
      "\n",
      "  [[ 0  0  0]\n",
      "   [ 0  0 -1]\n",
      "   [ 0  1  0]\n",
      "   [ 0  0  1]]\n",
      "\n",
      "  [[ 0 -1  0]\n",
      "   [-2  0  0]\n",
      "   [ 0  0  0]\n",
      "   [ 0  0  1]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example input image\n",
    "input_image = tf.keras.layers.Input(shape=(None, None, 3))  # Assuming RGB image\n",
    "\n",
    "# Define the size of the tiles\n",
    "tile_size = (4, 4, 3)  # Height, Width, Channels\n",
    "\n",
    "# Use tf.image.extract_patches to split the image into tiles\n",
    "tiles = tf.image.extract_patches(\n",
    "    input_image,\n",
    "    sizes=[1, tile_size[0], tile_size[1], 1],  # Batch, Height, Width, Channels\n",
    "    strides=[1, tile_size[0], tile_size[1], 1],  # Batch, Height, Width, Channels\n",
    "    rates=[1, 1, 1, 1],  # Batch, Height, Width, Channels\n",
    "    padding='VALID'\n",
    ")\n",
    "\n",
    "# Reshape the tiles to have the desired shape (4x4 tiles)\n",
    "tiles = tf.reshape(tiles, (-1, tile_size[0], tile_size[1], tile_size[2]))\n",
    "\n",
    "# Creating a model to visualize the result\n",
    "model = tf.keras.Model(inputs=input_image, outputs=tiles)\n",
    "model.summary()\n",
    "\n",
    "# Example usage with a random image\n",
    "random_image = tf.random.normal((1, 16, 16, 3))  # Assuming a 16x16 RGB image\n",
    "result_tiles = model.predict(random_image)\n",
    "\n",
    "# Displaying the result\n",
    "print(\"Input Image:\")\n",
    "print(random_image.numpy().squeeze().astype(int))\n",
    "print(\"\\nResult Tiles:\")\n",
    "print(result_tiles.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1ca81",
   "metadata": {},
   "source": [
    "###### GRAPH DATA STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52ab0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Graph:\n",
      "B -> B, D, C\n",
      "D -> C\n",
      "E -> A, C\n",
      "A -> A\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "\n",
    "    def add_edge(self, start, end):\n",
    "        if start not in self.graph:\n",
    "            self.graph[start] = []\n",
    "        self.graph[start].append(end)\n",
    "\n",
    "    def generate_random_connections(self, nodes, num_connections):\n",
    "        for _ in range(num_connections):\n",
    "            start = random.choice(nodes)\n",
    "            end = random.choice(nodes)\n",
    "            self.add_edge(start, end)\n",
    "\n",
    "    def display_graph(self):\n",
    "        for node, neighbors in self.graph.items():\n",
    "            print(f\"{node} -> {', '.join(neighbors)}\")\n",
    "\n",
    "# Example usage\n",
    "nodes = ['A', 'B', 'C', 'D', 'E']\n",
    "num_connections = 7\n",
    "\n",
    "graph = Graph()\n",
    "graph.generate_random_connections(nodes, num_connections)\n",
    "\n",
    "print(\"Random Graph:\")\n",
    "graph.display_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8e5209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "Initial Random Input: 90\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Node A Output: 180.0\n",
      "Next Node: High_Output_Node\n"
     ]
    }
   ],
   "source": [
    "# Custom Node Class\n",
    "class GraphNode:\n",
    "    def __init__(self, name, model):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "\n",
    "    def apply_inference_rule(self, input_data):\n",
    "        # Convert input data to NumPy arrays\n",
    "        input_a = np.array(input_data[0])\n",
    "        input_b = np.array(input_data[1])\n",
    "\n",
    "        # Apply inference rule based on the output\n",
    "        output = self.model.predict([input_a, input_b])\n",
    "        \n",
    "        # Assuming output is a NumPy array, access the first element\n",
    "        output_value = output[0]\n",
    "\n",
    "        if output_value > 50:\n",
    "            return \"High_Output_Node\"\n",
    "        elif output_value < 50:\n",
    "            return \"Low_Output_Node\"\n",
    "        else:\n",
    "            return \"Equal_Output_Node\"\n",
    "\n",
    "# Example usage\n",
    "nodes = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Create a graph with random connections\n",
    "graph = Graph()\n",
    "graph.generate_random_connections(nodes, 7)\n",
    "\n",
    "# Create a model using the CustomAddLayer\n",
    "input_a = tf.keras.layers.Input(shape=(1,))\n",
    "input_b = tf.keras.layers.Input(shape=(1,))\n",
    "add_result = CustomAddLayer(name='custom_add')([input_a, input_b])\n",
    "add_model = tf.keras.Model(inputs=[input_a, input_b], outputs=add_result)\n",
    "\n",
    "# Create nodes with custom models\n",
    "node_A = GraphNode(\"A\", add_model)\n",
    "node_B = GraphNode(\"B\", add_model)\n",
    "node_C = GraphNode(\"C\", add_model)\n",
    "node_D = GraphNode(\"D\", add_model)\n",
    "node_E = GraphNode(\"E\", add_model)\n",
    "\n",
    "# Simulate random input data for inference\n",
    "random_input = random.randint(1, 100)\n",
    "\n",
    "# Perform inference in Node A\n",
    "next_node = node_A.apply_inference_rule([[random_input], [random_input]])\n",
    "\n",
    "# Display the result\n",
    "print(f\"Initial Random Input: {random_input}\")\n",
    "print(f\"Node A Output: {add_model.predict([np.array([[random_input]]), np.array([[random_input]])])[0][0]}\")\n",
    "print(f\"Next Node: {next_node}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea2afd",
   "metadata": {},
   "source": [
    "###### OBJECT DETTECTION MODELS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97086de3",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = 'Jellyfish_image_dataset.zip'\n",
    "\n",
    "# Specify the directory where you want to extract the contents\n",
    "extract_dir = 'Object_Detection'\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Extract the contents of the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68cd0df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "453407f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 6 classes.\n",
      "Found 39 images belonging to 6 classes.\n",
      "Epoch 1/3\n",
      "45/45 [==============================] - 107s 2s/step - loss: 1.1908 - accuracy: 0.5922 - val_loss: 1.0807 - val_accuracy: 0.6410\n",
      "Epoch 2/3\n",
      "45/45 [==============================] - 97s 2s/step - loss: 0.4568 - accuracy: 0.8378 - val_loss: 1.0258 - val_accuracy: 0.6410\n",
      "Epoch 3/3\n",
      "45/45 [==============================] - 97s 2s/step - loss: 0.2856 - accuracy: 0.9044 - val_loss: 1.0878 - val_accuracy: 0.6154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2460b97fe50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Paths to your train and validation datasets\n",
    "train_dir = 'Train'\n",
    "validation_dir = 'Valid'\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 20\n",
    "\n",
    "# Create image data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load pre-trained MobileNetV2 model without top classification layer\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "model1 = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model1.fit(\n",
    "    train_generator,\n",
    "    epochs=3,\n",
    "    validation_data=validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6c4f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 6 classes.\n",
      "Found 39 images belonging to 6 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 12s 0us/step\n",
      "Epoch 1/3\n",
      "36/36 [==============================] - 1373s 38s/step - loss: 1.2410 - accuracy: 0.5389 - val_loss: 0.8535 - val_accuracy: 0.7692\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 1350s 38s/step - loss: 0.6216 - accuracy: 0.7778 - val_loss: 0.7757 - val_accuracy: 0.7949\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 1242s 35s/step - loss: 0.3920 - accuracy: 0.8756 - val_loss: 0.7708 - val_accuracy: 0.8205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2469eca7710>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Paths to your train and validation datasets\n",
    "train_dir = 'Train'\n",
    "validation_dir = 'Valid'\n",
    "img_height, img_width = 299, 299  # InceptionV3 requires input size of (299, 299)\n",
    "batch_size = 25\n",
    "\n",
    "# Create image data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Load pre-trained InceptionV3 model without top classification layer\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "model2 = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model2.fit(\n",
    "    train_generator,\n",
    "    epochs=3,\n",
    "    validation_data=validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a0d8c",
   "metadata": {},
   "source": [
    "###### MobileNetV2 & InceptionV3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76586f77",
   "metadata": {},
   "source": [
    "MobileNetV2 and InceptionV3 are both convolutional neural network (CNN) architectures designed for computer vision tasks, but they have differences in terms of architecture, speed, and accuracy. Here's a brief comparison:\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "MobileNetV2: MobileNetV2 is designed specifically for mobile and embedded vision applications. It uses depthwise separable convolutions to reduce the number of parameters and computations. \n",
    "It has a lightweight structure, making it suitable for real-time applications on devices with limited computational resources.\n",
    "\n",
    "InceptionV3: InceptionV3, also known as GoogLeNetV3, is part of the Inception family of models developed by Google. It uses a more complex architecture with inception modules, which are designed to capture features at different scales using parallel convolutions with different kernel sizes. InceptionV3 typically has a larger number of parameters compared to MobileNetV2.\n",
    "\n",
    "Speed and Computational Efficiency:\n",
    "\n",
    "MobileNetV2: MobileNetV2 is known for its computational efficiency and speed. It achieves a good balance between accuracy and model size, making it suitable for real-time applications on resource-constrained devices.\n",
    "\n",
    "InceptionV3: InceptionV3 is a deeper and more computationally intensive model compared to MobileNetV2. While it achieves higher accuracy, it may be slower and requires more computational resources.\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "MobileNetV2: MobileNetV2 is designed to be a lightweight model, and its primary focus is on efficiency. \n",
    "While it may not achieve state-of-the-art accuracy on some tasks, it provides a good trade-off between speed and accuracy.\n",
    "\n",
    "InceptionV3: InceptionV3 generally achieves higher accuracy on various computer vision tasks. \n",
    "It is suitable for scenarios where achieving the highest possible accuracy is more critical than computational efficiency.\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "MobileNetV2: Ideal for mobile and edge devices where computational resources are limited, and real-time performance is crucial. Commonly used in applications such as image classification, object detection, and segmentation on mobile devices.\n",
    "\n",
    "InceptionV3: Suitable for tasks where higher accuracy is required, and computational resources are less constrained. \n",
    "Often used in applications like image recognition, fine-grained classification, and image analysis.\n",
    "\n",
    "\n",
    "Issues faced: While performing this section, I could use only certain models to train and validate. \n",
    "Also it is too slow to perform. So I couln't use more epochs and do hyperparameter tuning properly. consider the consequences while I'm solving out the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2af53a",
   "metadata": {},
   "source": [
    "###### QUANTIZED MODEL FOR MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b13ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x00000246F1069080> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function trace_model_call.<locals>._wrapped_model at 0x00000246F1069080>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x00000246F1069080> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function trace_model_call.<locals>._wrapped_model at 0x00000246F1069080>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x00000246F1069080> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function trace_model_call.<locals>._wrapped_model at 0x00000246F1069080>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input, resource with unsupported characters which will be renamed to mobilenetv2_1_00_224_input, sequential_3_dense_7_biasadd_readvariableop_resource in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002470219BCE0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002470219BCE0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002470219BCE0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002470219BCE0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002470219BCE0> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002470219BCE0>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpja9rd231\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpja9rd231\\assets\n",
      "INFO:absl:Writing fingerprint to C:\\Users\\user\\AppData\\Local\\Temp\\tmpja9rd231\\fingerprint.pb\n",
      "INFO:absl:Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to a quantized model \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model1)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_model1 = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6130254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 6 classes.\n",
      "Accuracy: 0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Paths to your test dataset\n",
    "test_dir = 'Valid'  # Replace with the actual path to your test dataset\n",
    "\n",
    "# Create an image data generator for the test dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,  # Set batch_size to 1 for inference on individual images\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Ensure the order of predictions matches the order of images\n",
    ")\n",
    "\n",
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_model1)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Perform inference on each image in the test dataset\n",
    "all_predictions = []\n",
    "\n",
    "ground_truth_labels = test_generator.classes  # Ground truth labels (integer classes)\n",
    "\n",
    "for i in range(len(test_generator.filenames)):\n",
    "    # Load an image for inference\n",
    "    image_path = os.path.join(test_dir, test_generator.filenames[i])\n",
    "    image = Image.open(image_path).resize((img_width, img_height))\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "\n",
    "    # Prepare the input data\n",
    "    input_data = np.expand_dims(image, axis=0).astype(input_details[0]['dtype'])\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the output\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predicted_class = np.argmax(output_data)\n",
    "    all_predictions.append(predicted_class)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.sum(np.array(all_predictions) == ground_truth_labels)\n",
    "total_samples = len(ground_truth_labels)\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "# Print or use the accuracy as needed\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fe973",
   "metadata": {},
   "source": [
    "###### QUANTIZED MODEL FOR Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0540945c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RetinaNet' object has no attribute 'call'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m converter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mTFLiteConverter\u001b[38;5;241m.\u001b[39mfrom_keras_model(model2)\n\u001b[0;32m      3\u001b[0m converter\u001b[38;5;241m.\u001b[39moptimizations \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mOptimize\u001b[38;5;241m.\u001b[39mDEFAULT]\n\u001b[1;32m----> 4\u001b[0m quantized_model2 \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save the quantized model to a file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantized_model2.tflite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1065\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1042\u001b[0m, in \u001b[0;36m_convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1531\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:215\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[0m, in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1468\u001b[0m, in \u001b[0;36m_freeze_keras_model\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RetinaNet' object has no attribute 'call'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert the new model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_model2 = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open('quantized_model2.tflite', 'wb') as f:\n",
    "    f.write(quantized_model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7f745a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_model_maker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtflite_model_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_spec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtflite_model_maker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_classifier\n\u001b[0;32m      4\u001b[0m model_spec \u001b[38;5;241m=\u001b[39m model_spec\u001b[38;5;241m.\u001b[39mImageModelSpec\u001b[38;5;241m.\u001b[39mfrom_keras_model(model2)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflite_model_maker'"
     ]
    }
   ],
   "source": [
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "\n",
    "model_spec = model_spec.ImageModelSpec.from_keras_model(model2)\n",
    "image_classifier.create(model_spec, train_data=train_generator, validation_data=validation_generator)\n",
    "quantized_model2 = model_spec.create_tflite(quantization_config='dr')\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open('quantized_model2.tflite', 'wb') as f:\n",
    "    f.write(quantized_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Paths to your test dataset\n",
    "test_dir = 'Valid'  # Replace with the actual path to your test dataset\n",
    "\n",
    "# Create an image data generator for the test dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=1,  # Set batch_size to 1 for inference on individual images\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Ensure the order of predictions matches the order of images\n",
    ")\n",
    "\n",
    "# Load the quantized TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_model2)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Perform inference on each image in the test dataset\n",
    "all_predictions = []\n",
    "\n",
    "ground_truth_labels = test_generator.classes  # Ground truth labels (integer classes)\n",
    "\n",
    "for i in range(len(test_generator.filenames)):\n",
    "    # Load an image for inference\n",
    "    image_path = os.path.join(test_dir, test_generator.filenames[i])\n",
    "    image = Image.open(image_path).resize((img_width, img_height))\n",
    "    image = np.array(image) / 255.0  # Normalize the image\n",
    "\n",
    "    # Prepare the input data\n",
    "    input_data = np.expand_dims(image, axis=0).astype(input_details[0]['dtype'])\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the output\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predicted_class = np.argmax(output_data)\n",
    "    all_predictions.append(predicted_class)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.sum(np.array(all_predictions) == ground_truth_labels)\n",
    "total_samples = len(ground_truth_labels)\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "# Print or use the accuracy as needed\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89ee27",
   "metadata": {},
   "source": [
    "###### DIFFERENCES"
   ]
  },
  {
   "cell_type": "raw",
   "id": "342b59a7",
   "metadata": {},
   "source": [
    "The key difference between quantized models and full precision (FP32) models lies in the representation of numerical values. Quantization is a technique used to reduce the memory and computational requirements of a model by representing weights and activations with fewer bits. FP32 models use 32-bit floating-point precision for numerical values, while quantized models use a lower bit precision.\n",
    "\n",
    "The primary differences between quantized models and FP32 models:\n",
    "\n",
    "1. Precision of Numerical Values:\n",
    "\n",
    "FP32 Model (Full Precision):\n",
    "Weights and activations are represented using 32-bit floating-point numbers.\n",
    "High precision allows for a wide range of values and fine-grained representation of numerical information.\n",
    "Requires more memory and computational resources.\n",
    "\n",
    "Quantized Model:\n",
    "Weights and activations are represented using a lower bit precision, typically 8-bit integers (INT8) or even lower.\n",
    "Lower precision reduces memory requirements and speeds up inference but sacrifices some level of numerical precision.\n",
    "\n",
    "2. Memory Requirements:\n",
    "\n",
    "FP32 Model:\n",
    "Requires more memory due to the larger size of 32-bit floating-point numbers.\n",
    "Higher memory requirements can be a limiting factor, especially on resource-constrained devices.\n",
    "\n",
    "Quantized Model:\n",
    "Requires less memory as a result of using lower precision for weights and activations.\n",
    "Well-suited for deployment on devices with limited memory.\n",
    "\n",
    "3. Computational Efficiency:\n",
    "\n",
    "FP32 Model:\n",
    "More computationally intensive due to the higher precision calculations.\n",
    "Slower inference on devices with limited computational power.\n",
    "\n",
    "Quantized Model:\n",
    "Faster inference due to reduced precision and lower computational requirements.\n",
    "Well-suited for deployment on edge devices with constrained computational resources.\n",
    "\n",
    "4. Deployment Considerations:\n",
    "\n",
    "FP32 Model:\n",
    "Often used during the training and development phase for maximum numerical precision and accuracy.\n",
    "Larger model sizes may be a concern for deployment on devices with limited storage.\n",
    "\n",
    "Quantized Model:\n",
    "Commonly used for deployment on edge devices, mobile devices, or IoT devices where memory and computational resources are limited.\n",
    "Reduced precision may lead to a slight drop in accuracy, but the trade-off is often acceptable for the benefits in terms of speed and resource efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742cccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1677da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
